# -*- coding: utf-8 -*-
"""PALAK_UMESH_MALLAWAT_hw4_trying.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FNanhW0MffBSAyEZhvgkH7Shjfz2xB7D
"""

# import pandas as pd
# from google.colab import drive
# drive.mount('/content/drive')

import sys





train = sys.argv[1]
dev = sys.argv[2]
test = sys.argv[3]
glove = sys.argv[4]
print(train,dev,test,glove)

# print("Argument 1:", arg1)
# print("Argument 2:", arg2)


#Importing necessary libraries 
import numpy as np
import pandas as pd
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR
import random
import json
torch.manual_seed(0)
random.seed(0)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""## TASK 1"""

#for train dataset
train_df = []
with open(train, 'r') as f:
    for line in f.readlines():
        if len(line) > 2:
            id, word, ner_tag = line.strip().split(" ")
            train_df.append([id, word, ner_tag])

train_df = pd.DataFrame(train_df, columns=['id', 'word', 'NER'])
train_df = train_df.dropna()

train_x, train_y = [], []
x, y = [], []
for i, row in enumerate(train_df.itertuples()):
  # print(i,row)
  if i > 0 and row.id == '1':
    # print("in loop")
    train_x.append(x)
    train_y.append(y)
    x, y = [], []
  x.append(row.word)
  y.append(row.NER)
train_x.append(x)
train_y.append(y)
print(len(train_x), len(train_y))

#for dev dataset
dev_df = []
with open(dev, 'r') as f:
    for line in f.readlines():
        if len(line) > 2:
            id, word, ner_tag = line.strip().split(" ")
            dev_df.append([id, word, ner_tag])

dev_df = pd.DataFrame(dev_df, columns=['id', 'word', 'NER'])
dev_df = dev_df.dropna()

train_dev_x, train_dev_y = [], []
x, y = [], []
for i, row in enumerate(dev_df.itertuples()):
  # print(i,row)
  if i > 0 and row.id == '1':
    # print("in loop")
    train_dev_x.append(x)
    train_dev_y.append(y)
    x, y = [], []
  x.append(row.word)
  y.append(row.NER)
train_dev_x.append(x)
train_dev_y.append(y)
print(len(train_dev_x), len(train_dev_y))

#for test data set
test_df = []
with open(test, 'r') as f:
    for line in f.readlines():
        if len(line) > 1:
            id, word = line.strip().split(" ")
            test_df.append([id, word])

test_df = pd.DataFrame(test_df, columns=['id', 'word'])
test_df = test_df.dropna()

train_test_x = []
x= []
for i, row in enumerate(test_df.itertuples()):
  if i > 0 and row.id == '1':
    train_test_x.append(x)
    # train_y.append(y)
    x = []
  x.append(row.word)
  # y.append(row.NER)
train_test_x.append(x)
# train_y.append(y)

print(len(train_test_x))

"""# TASK 1"""

with open('./vocab.json') as json_file:
    word_index = json.load(json_file)

with open('./label_dict.json') as json_file:
    label_dict = json.load(json_file)
rev_label_dict = {v: k for k, v in label_dict.items()}

# word_index = {"<PAD>": 0, "<UNK>": 1}
# index = 2
# train_scent=[train_x, train_dev_x, train_test_x]
# for data in train_scent:
#   # print(data)
#   for scent in data:
#     # print(sent)
#     for word in scent:
#       if word not in word_index:
#         word_index[word] = index
#         index += 1
# with open("vocab.json", "w") as outfile:
#   json.dump(word_index,outfile)


#creating vectors for scentences

train_x_vec = []
tmp_vec = []
for words in train_x:
    for word in words:
        tmp_vec.append(word_index[word])
    train_x_vec.append(tmp_vec)
    tmp_vec = []

dev_x_vec = []
tmp_vec1 = []
for words in train_dev_x:
    for word in words:
        tmp_vec1.append(word_index[word])
    dev_x_vec.append(tmp_vec1)
    tmp_vec1 = []

test_x_vec = []
tmp_vec = []
for words in train_test_x:
    for word in words:
        tmp_vec.append(word_index[word])
    test_x_vec.append(tmp_vec)
    tmp_vec = []

# import json
# labels = set()
# label_dict = {}
# rev_label_dict = {}
# index=0

# # for x in [train_y, train_dev_y]:
# #     for sentence in x:
# #         for label in sentence:
# #             labels.add(label)
# #             if label not in label_dict:
# #                 label_dict[label] = index
# #                 rev_label_dict[index] = label
# #                 index+=1
# # print(label_dict)
# # print(rev_label_dict)


# label_dict={'I-MISC':0,'B-MISC':1,'B-LOC':2,'I-PER':3,'B-ORG':4,'B-PER':5,'O':6,'I-LOC':7,'I-ORG':8}
# rev_label_dict = {v: k for k, v in label_dict.items()}
# print(label_dict)
# print(rev_label_dict)
# with open("label_dict.json", "w") as outfile:
#   json.dump(label_dict,outfile)

train_y_vec = []
for seq in train_y:
    vector = [label_dict[label] for label in seq]
    train_y_vec.append(vector)


dev_y_vec = []
for seq in train_dev_y:
    vector = [label_dict[label] for label in seq]
    dev_y_vec.append(vector)

import math
import torch
class_counts = [0] * len(label_dict)
for tags in train_y + train_dev_y:
    for tag in tags:
        class_id = label_dict.get(tag)
        if class_id is not None:
            class_counts[class_id] += 1

total_count = sum(class_counts)
class_wt = []
for count in class_counts:
    if count == 0:
        weight = 1.0
    else:
        weight = 0.35 * total_count / count
        score = round(math.log(weight), 2)
        weight = max(score, 1.0)
        
    class_wt.append(weight)
class_wt = torch.tensor(class_wt)
# print(class_wt)

class BiLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,
                 bidirectional, dropout_val, tag_size):
        super(BiLSTM, self).__init__()
        """ Hyper Parameters """
        self.hidden_dim = hidden_dim  # hidden_dim = 256
        self.lstm_layers = lstm_layers  # LSTM Layers = 1
        self.embedding_dim = embedding_dim  # Embedding Dimension = 100
        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128
        self.tag_size = tag_size  # Tag Size = 9
        self.num_directions = 2 

        """ Initializing Network """
        self.embedding = nn.Embedding(
            vocab_size, embedding_dim)  # Embedding Layer
        self.embedding.weight.data.uniform_(-1,1)
        self.LSTM = nn.LSTM(embedding_dim,
                            hidden_dim,
                            num_layers=lstm_layers,
                            batch_first=True,
                            bidirectional=True)
        self.fc = nn.Linear(hidden_dim*self.num_directions,
                            linear_out_dim)  # 2 for bidirection
        self.dropout = nn.Dropout(dropout_val)
        self.elu = nn.ELU(alpha=0.01)
        self.classifier = nn.Linear(linear_out_dim, self.tag_size)

    def init_hidden(self, batch_size):
        h, c = (torch.zeros(self.lstm_layers * self.num_directions,
                            batch_size, self.hidden_dim).to(device),
                torch.zeros(self.lstm_layers * self.num_directions,
                            batch_size, self.hidden_dim).to(device))
        return h, c

    def forward(self, sen, sen_len):  # sen_len
        # Set initial states
        batch_size = sen.shape[0]
        h_0, c_0 = self.init_hidden(batch_size)

        # Forward propagate LSTM
        embedded = self.embedding(sen).float()
        packed_embedded = pack_padded_sequence(
            embedded, sen_len, batch_first=True, enforce_sorted=False)
        output, _ = self.LSTM(packed_embedded, (h_0, c_0))
        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)
        dropout = self.dropout(output_unpacked)
        lin = self.fc(dropout)
        pred = self.elu(lin)
        pred = self.classifier(pred)
        return pred

class CustomBiLSTMDataLoader(Dataset):
    def __init__(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, index):
        x_instance = torch.tensor(self.inputs[index])  # , dtype=torch.long
        y_instance = torch.tensor(self.targets[index])  # , dtype=torch.float
        return x_instance, y_instance


class CustomBiLSTMTestLoader(Dataset):
    def __init__(self, inputs):
        self.inputs = inputs

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, index):
        x_instance = torch.tensor(self.inputs[index])  # , dtype=torch.long
        # y_instance = torch.tensor(self.y[index])  # , dtype=torch.float
        return x_instance



class CustomCollator(object):

    def __init__(self, vocab, label):
        self.params = vocab
        self.label = label

    def __call__(self, batch):
        (xx, yy) = zip(*batch)
        x_len = [len(x) for x in xx]
        y_len = [len(y) for y in yy]
        batch_max_len = max([len(s) for s in xx])
        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))
        batch_labels = -1*np.zeros((len(xx), batch_max_len))
        for j in range(len(xx)):
            cur_len = len(xx[j])
            batch_data[j][:cur_len] = xx[j]
            batch_labels[j][:cur_len] = yy[j]

        batch_data, batch_labels = torch.LongTensor(
            batch_data), torch.LongTensor(batch_labels)
        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)

        return batch_data, batch_labels, x_len, y_len


class CustomTestCollator(object):

    def __init__(self, vocab, label):
        self.params = vocab
        self.label = label

    def __call__(self, batch):
        xx = batch
        x_len = [len(x) for x in xx]
        # y_len = [len(y) for y in yy]
        batch_max_len = max([len(s) for s in xx])
        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))
        # batch_labels = -1*np.zeros((len(xx), batch_max_len))
        for j in range(len(xx)):
            cur_len = len(xx[j])
            batch_data[j][:cur_len] = xx[j]
            # batch_labels[j][:cur_len] = yy[j]

        batch_data = torch.LongTensor(batch_data)
        batch_data = Variable(batch_data)

        return batch_data, x_len

# BiLSTM_model = BiLSTM(vocab_size=len(word_index),
#                       embedding_dim=100,
#                       linear_out_dim=128,
#                       hidden_dim=256,
#                       lstm_layers=1,
#                       bidirectional=True,
#                       dropout_val=0.33,
#                       tag_size=len(label_dict))
# # BiLSTM_model.load_state_dict(torch.load("./BiLSTM_epoch_10.pt"))
# BiLSTM_model.to(device)
# print(BiLSTM_model)

# BiLSTM_train = CustomBiLSTMDataLoader(train_x_vec, train_y_vec)
# custom_collator = CustomCollator(word_index, label_dict)
# dataloader = DataLoader(dataset=BiLSTM_train,
#                         batch_size=4,
#                         drop_last=True,
#                         collate_fn=custom_collator)

# criterion = nn.CrossEntropyLoss(weight=class_wt)
# # criterion = nn.NLLLoss(weight=class_wt)
# # criterion = loss_fn
# criterion = criterion.to(device)
# criterion.requres_grad = True
# optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)
# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min")
# #scheduler = StepLR(optimizer, step_size=15, gamma=0.9)
# epochs = 200

# for i in range(1, epochs+1):
#     train_loss = 0.0
#     # scheduler.step(train_loss)
#     for input, label, input_len, label_len in dataloader:
#         optimizer.zero_grad()
#         output = BiLSTM_model(input.to(device), input_len)  # input_len
#         output = output.view(-1, len(label_dict))
#         label = label.view(-1)
#         loss = criterion(output, label.to(device))
#         # print(loss)
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item() * input.size(1)

#     train_loss = train_loss / len(dataloader.dataset)
#     print('Epoch: {} \tTraining Loss: {:.6f}'.format(i, train_loss))
#     torch.save(BiLSTM_model.state_dict(),
#                'drive/My Drive/hw4/BiLSTM_epoch_' + str(i) + '.pt')

BiLSTM_model = BiLSTM(vocab_size=len(word_index),
                      embedding_dim=100,
                      linear_out_dim=128,
                      hidden_dim=256,
                      lstm_layers=1,
                      bidirectional=True,
                      dropout_val=0.33,
                      tag_size=len(label_dict))

BiLSTM_model.load_state_dict(torch.load("./blstm1.pt"))
BiLSTM_model.to(device)

#tesing on validation data
# rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in word_index.items()}
BiLSTM_dev = CustomBiLSTMDataLoader(dev_x_vec, dev_y_vec)
custom_collator = CustomCollator(word_index, label_dict)
dataloader_dev = DataLoader(dataset=BiLSTM_dev,batch_size=1,shuffle=False,drop_last=True,collate_fn=custom_collator)
file = open("dev1_train_test.out", 'w')
for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

    pred = BiLSTM_model(dev_data.to(device), dev_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    label = label.detach().numpy()
    dev_data = dev_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(label), -1))

    for i in range(len(dev_data)):
        for j in range(len(dev_data[i])):
            if dev_data[i][j] != 0:
                word = rev_vocab_dict[dev_data[i][j]]
                gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j+1), word, gold, op]))
                file.write("\n")
        file.write("\n")

file.close()

# !perl 'drive/My Drive/conll03eval.txt' < '/content/dev1_train_test.out'

#Testing on Testing Dataset 
# rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in word_index.items()}

BiLSTM_test = CustomBiLSTMTestLoader(test_x_vec)
custom_test_collator = CustomTestCollator(word_index, label_dict)
dataloader_test = DataLoader(dataset=BiLSTM_test,
                                batch_size=1,
                                shuffle=False,
                                drop_last=True,
                                collate_fn=custom_test_collator)


file = open("test1.out", 'w')
for test_data, test_data_len in dataloader_test:

    pred = BiLSTM_model(test_data.to(device), test_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    test_data = test_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(test_data), -1))
    
    for i in range(len(test_data)):
        for j in range(len(test_data[i])):
            if test_data[i][j] != 0:
                word = rev_vocab_dict[test_data[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j+1), word, op]))
                file.write("\n")

        file.write("\n")
        
file.close()

#tesing on validation data
# rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in word_index.items()}
BiLSTM_dev = CustomBiLSTMDataLoader(dev_x_vec, dev_y_vec)
custom_collator = CustomCollator(word_index, label_dict)
dataloader_dev = DataLoader(dataset=BiLSTM_dev,batch_size=1,shuffle=False,drop_last=True,collate_fn=custom_collator)
file = open("dev1.out", 'w')
for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

    pred = BiLSTM_model(dev_data.to(device), dev_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    label = label.detach().numpy()
    dev_data = dev_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(label), -1))

    for i in range(len(dev_data)):
        for j in range(len(dev_data[i])):
            if dev_data[i][j] != 0:
                word = rev_vocab_dict[dev_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j+1), word, op]))
                file.write("\n")
        file.write("\n")

file.close()

"""# Task 2"""

def create_emb_matrix(word_idx, emb_dict, dimension):

    emb_matrix = [emb_dict[word] if word in emb_dict else 
                  emb_dict[word.lower()] + 5e-3 if word.lower() in emb_dict else 
                  emb_dict["<UNK>"]
                  for word, idx in word_idx.items()]

    return np.array(emb_matrix)

class BiLSTM_glove(nn.Module):
    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,
                 bidirectional, dropout_val, tag_size, emb_matrix):
        super(BiLSTM_glove, self).__init__()
        """ Hyper Parameters """
        self.hidden_dim = hidden_dim  # hidden_dim = 256
        self.lstm_layers = lstm_layers  # LSTM Layers = 1
        self.embedding_dim = embedding_dim  # Embedding Dimension = 100
        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128
        self.tag_size = tag_size  # Tag Size = 9
        self.emb_matrix = emb_matrix
        self.num_directions = 2 if bidirectional else 1

        """ Initializing Network """
        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding Layer
        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))
        self.LSTM = nn.LSTM(embedding_dim,
                            hidden_dim,
                            num_layers=lstm_layers,
                            batch_first=True,
                            bidirectional=True)
        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)  # 2 for bidirection
        self.dropout = nn.Dropout(dropout_val)
        self.elu = nn.ELU(alpha=0.01)
        self.classifier = nn.Linear(linear_out_dim, self.tag_size)

    def init_hidden(self, batch_size):
        h, c = (torch.zeros(self.lstm_layers * self.num_directions,
                            batch_size, self.hidden_dim).to(device),
                torch.zeros(self.lstm_layers * self.num_directions,
                            batch_size, self.hidden_dim).to(device))
        return h, c

    def forward(self, sen, sen_len):  # sen_len
        # Set initial states
        batch_size = sen.shape[0]
        h_0, c_0 = self.init_hidden(batch_size)

        # Forward propagate LSTM
        embedded = self.embedding(sen).float()
        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)
        output, _ = self.LSTM(packed_embedded, (h_0, c_0))
        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)
        dropout = self.dropout(output_unpacked)
        lin = self.fc(dropout)
        pred = self.elu(lin)
        pred = self.classifier(pred)
        return pred

word_index = {"<PAD>": 0, "<UNK>": 1}
index = 2
train_scent=[train_x, train_dev_x, train_test_x]
for data in train_scent:
  # print(data)
  for scent in data:
    # print(sent)
    for word in scent:
      if word not in word_index:
        word_index[word] = index
        index += 1

# label_dict = {'I-MISC', 'I-ORG', 'B-PER', 'B-LOC', 'O', 'I-PER', 'B-ORG', 'I-LOC', 'B-MISC'}
# rev_label_dict = {'I-MISC': 0, 'I-ORG': 1, 'B-PER': 2, 'B-LOC': 3, 'O': 4, 'I-PER': 5, 'B-ORG': 6, 'I-LOC': 7, 'B-MISC': 8}

# Load GloVe embeddings
glove = pd.read_csv(glove, sep=" ", quoting=3, header=None, index_col=0)
glove_emb = {key: val.values for key, val in glove.T.items()}

# Add PAD and UNK tokens
glove_vec = np.array([glove_emb[key] for key in glove_emb])
glove_emb["<PAD>"] = np.zeros((100,), dtype="float64")
glove_emb["<UNK>"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)

# Create embedding matrix
emb_matrix = create_emb_matrix(word_index,glove_emb,100)

# Print vocabulary and embedding dimensions
vocab_size, vector_size = emb_matrix.shape
print(vocab_size, vector_size)

# # Define BiLSTM model architecture
# BiLSTM_model = BiLSTM_glove(
#     vocab_size=emb_matrix.shape[0],
#     embedding_dim=100,
#     linear_out_dim=128,
#     hidden_dim=256,
#     lstm_layers=1,
#     bidirectional=True,
#     dropout_val=0.33,
#     tag_size=len(label_dict),
#     emb_matrix=emb_matrix
# )
# BiLSTM_model.to(device)

# # Create data loader for training data
# train_data = CustomBiLSTMDataLoader(train_x_vec, train_y_vec)
# collator = CustomCollator(word_index, label_dict)
# dataloader = DataLoader(
#     dataset=train_data,
#     batch_size=8,
#     drop_last=True,
#     collate_fn=collator
# )

# # Define loss function and optimizer
# criterion = nn.CrossEntropyLoss(weight=class_wt)
# criterion = criterion.to(device)
# optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)
# scheduler = StepLR(optimizer, step_size=15, gamma=0.9)

# # Train the model
# num_epochs = 50
# for epoch in range(num_epochs):
#     train_loss = 0.0
#     for input, label, input_len, label_len in dataloader:
#         optimizer.zero_grad()
#         # input = input.float().to(device)
#         output = BiLSTM_model(input.to(device), input_len)
#         output = output.view(-1, len(label_dict))
#         label = label.view(-1)
#         loss = criterion(output, label.to(device))
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item() * input.size(1)
#     train_loss /= len(dataloader.dataset)
#     print(f'Epoch: {epoch+1}\tTraining Loss: {train_loss:.6f}')
#     torch.save(BiLSTM_model.state_dict(), f'drive/My Drive/hw4/BiLSTM_glove_{epoch+1}.pt')

BiLSTM_model = BiLSTM_glove(vocab_size=len(word_index),
                        embedding_dim=100,
                        linear_out_dim=128,
                        hidden_dim=256,
                        lstm_layers=1,
                        bidirectional = True,
                        dropout_val=0.33,
                        tag_size=len(label_dict),
                        emb_matrix=emb_matrix)

BiLSTM_model.load_state_dict(torch.load("./blstm2.pt"))
BiLSTM_model.to(device)

#predicting for validation dataset
rev_vocab_dict = dict(map(reversed, word_index.items()))
# rev_label_dict = dict(map(reversed, label_dict.items()))

BiLSTM_dev = CustomBiLSTMDataLoader(dev_x_vec, dev_y_vec)
custom_collator = CustomCollator(word_index, label_dict)
dataloader_dev = DataLoader(dataset=BiLSTM_dev,
                            batch_size=8,
                            shuffle=False,
                            drop_last=True,
                            collate_fn=custom_collator)
# print(label_dict)
# rev_label_dict = {v: k for k, v in label_dict.items()}
# rev_vocab_dict = {v: k for k, v in word_index.items()}

res = []
file = open("dev2_train_test.out", 'w')
for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

    pred = BiLSTM_model(dev_data.to(device), dev_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    label = label.detach().numpy()
    dev_data = dev_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(label), -1))

    for i in range(len(dev_data)):
        for j in range(len(dev_data[i])):
            if dev_data[i][j] != 0:
                word = rev_vocab_dict[dev_data[i][j]]
                gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                res.append((word, gold, op))
                file.write(" ".join([str(j + 1), word, gold, op]))
                file.write("\n")
        file.write("\n")
file.close()

# !perl 'drive/My Drive/conll03eval.txt' < 'dev2_train_test.out'

#predicting for testing dataset
rev_vocab_dict = dict(map(reversed, word_index.items()))
# rev_label_dict = dict(map(reversed, label_dict.items()))
BiLSTM_test = CustomBiLSTMTestLoader(test_x_vec)
custom_test_collator = CustomTestCollator(word_index, label_dict)
dataloader_test = DataLoader(dataset=BiLSTM_test,
                                batch_size=1,
                                shuffle=False,
                                drop_last=True,
                                collate_fn=custom_test_collator)


res = []
file = open("test2.out", 'w')
for test_data, test_data_len in dataloader_test:

    pred = BiLSTM_model(test_data.to(device), test_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    # label = label.detach().numpy()
    test_data = test_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(test_data), -1))

    for i in range(len(test_data)):
        for j in range(len(test_data[i])):
            if test_data[i][j] != 0:
                word = rev_vocab_dict[test_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                res.append((word, op))
                file.write(" ".join([str(j + 1), word, op]))
                file.write("\n")
        file.write("\n")
file.close()

#predicting for validation dataset
rev_vocab_dict = dict(map(reversed, word_index.items()))
# rev_label_dict = dict(map(reversed, label_dict.items()))

BiLSTM_dev = CustomBiLSTMDataLoader(dev_x_vec, dev_y_vec)
custom_collator = CustomCollator(word_index, label_dict)
dataloader_dev = DataLoader(dataset=BiLSTM_dev,
                            batch_size=8,
                            shuffle=False,
                            drop_last=True,
                            collate_fn=custom_collator)
# print(label_dict)
# rev_label_dict = {v: k for k, v in label_dict.items()}
# rev_vocab_dict = {v: k for k, v in word_index.items()}

res = []
file = open("dev2.out", 'w')
for dev_data, label, dev_data_len, label_data_len in dataloader_dev:

    pred = BiLSTM_model(dev_data.to(device), dev_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    label = label.detach().numpy()
    dev_data = dev_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(label), -1))

    for i in range(len(dev_data)):
        for j in range(len(dev_data[i])):
            if dev_data[i][j] != 0:
                word = rev_vocab_dict[dev_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                res.append((word, gold, op))
                file.write(" ".join([str(j + 1), word, op]))
                file.write("\n")
        file.write("\n")
file.close()

